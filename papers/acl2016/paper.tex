\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{amsmath}
\usepackage{times}
\usepackage{latexsym}
\usepackage{color}

\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\mattnote}[1]{\textcolor{red}{NOTE: #1}}
\newcommand{\blank}{\_\_\_}
\newcommand{\predicate}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\entity}[1]{\ensuremath{\mathrm{#1}}}

\newcommand{\true}[0]{\predicate{true}}
\newcommand{\false}[0]{\predicate{false}}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

\title{Combining formal and distributional models of predicate semantics}

\author{}%Matt Gardner and Jayant Krishnamurthy\\
%Allen Institute for Artificial Intelligence\\
%Seattle, Washington, USA\\
%{\tt \{mattg,jayantk\}@allenai.org}}

\date{}

\begin{document}

\maketitle

\begin{abstract}

  We consider the problem of learning models of predicate (category and
  relation) semantics using both distributional information and structured
  information contained in a formal knowledge base.  Most previous approaches
  to modeling predicate semantics have either been purely distributional (such
  as word2vec and knowledge base embedding methods) or purely formal (such as
  most semantic parsers).  Distributional approaches have very broad coverage,
  but lack the compositional, logical semantics used by semantic parsers, and
  it has so far been difficult to combine the benefits of both of these
  techniques.

  In this paper, we present a method that incorporates logical statements
  derived from a knowledge base into distributional models of categories and
  relations.  Instead of mapping a phrase to a single logical statement, as
  done by semantic parsers, our method uses logical statements as features
  added to distributional models of words, allowing the model to learn that
  Freebase entities that are "honchos" are people, not buildings, and that they
  are frequently CEOs or film producers.  We use these models in an
  open-vocabulary semantic parsing task, showing a 63\% relative improvement in
  mean average precision over a distributional-only model, and a X\%
  improvement over a formal-only approach.

\end{abstract}

\section{Introduction}

How should a computer program represent the meaning of a word or a phrase?  The
answer to this question has a long and rich history.  Motivated by
Harris's distributional
hypothesis~\cite{harris-1954-distributional-hypothesis}, much work has been put
into representing words as collections of the contexts in which they appear.
Recent work with neural networks and word embeddings have shown that these
representations can be very useful for a number of natural language processing
tasks. \mattnote{cite something here}

At the same time, other researchers have found Montague's ideas about English
as a formal language to be useful~\cite{montague-1974-english-formal-language}.
Modern semantic parsers make use of a model-theoretic semantics that represents
words and phrases as formal predicates.  These predicates are then typically
mapped onto a query over a structured knowledge base, such as
Freebase~\cite{freebase-2008-bollacker}.  This kind of modeling allows semantic
parsers to handle the compositional structure of natural language and to
leverage large, curated knowledge sources to perform various natural language
processing tasks.  \mattnote{cite some semantic parsing papers}

Both of these approaches to modeling predicate semantics have aspects that are
very appealing.  It is relatively straightforward to learn a distributional
vector for every word or phrase in a corpus, allowing easy generalization from
words labeled with part of speech tags or dependency information to other words
that are distributionally similar.  Yet distributional approaches struggle to
make good use of the structured information contained in curated knowledge
sources like Freebase and WordNet~\cite{wordnet-1995}.  On the other hand,
because formal approaches to semantics typically model word meaning in terms of
a structured knowledge source, their applicability is limited to the set of
concepts represented in that knowledge source---a semantic parser trained with
Freebase cannot even represent words whose meaning has no correlate in the
Freebase ontology, such as ``front-runner'' and ``honcho''.

In this work we introduce models of predicate semantics that combine the
benefits of both distributional and formal approaches.  While there is no
Freebase predicate that corresponds exactly to the meaning of ``honcho'',
Freebase \emph{does} contain many predicates that can \emph{partially}
represent the word.  Our models extract features from Freebase that are
correlated with words and phrases seen in natural language text, and learns
weights for those features as additional components of a distributional model
of the word's meaning.

To demonstrate these models, we focus on the task of fill-in-the-blank natural
language queries, such as ``Italian architect \blank{}''.  Similar to many
current semantic parsers, we first convert this natural language query into a
logical form: $\lambda x. \predicate{architect}(x) \land
\predicate{architect\_N/N}(\entity{Italian}, x)$.  A typical semantic
parser (e.g., that of Kwiatkowski et
al.~\shortcite{kwiatkowski-2013-ontology-matching}) would then try to map this
logical form to a Freebase query, matching \entity{Italian} to
(\predicate{/people/person/nationality}, \entity{Italy}) and
\predicate{architect} to (\predicate{/type/object/type},
\entity{/architecture/architect}).  In contrast, we follow Krishnamurthy and
Mitchell~\shortcite{krishnamurthy-2015-semparse-open-vocabulary} in learning a
model for each predicate that can give a \emph{score} to potential arguments.
However, while the models of Krishnamurthy and Mitchell are purely
distributional, using Freebase only as a means of finding entities in text, we
inject additional information from Freebase into the models for each predicate.
The model we learn for \predicate{architect} has a \emph{feature} capturing the
fact that an entity has the type \entity{/architecture/architect} in Freebase,
and the model for \predicate{architect\_N/N} has a \emph{feature} capturing
whether the first argument is the country of nationality of the second
argument.

This is a fundamentally new kind of predicate semantics.  It is
distributional---part of the model is still trained in a manner very similar to
word2vec---yet it is also formal.  However, instead of committing to a single
Freebase query to represent a phrase, as traditional formal approaches do, our
models are flexible, encoding many possible Freebase queries as features that
can give hints to the model.  Thus words that would be impossible to represent
as a single Freebase query can still benefit from the information contained in
Freebase---our model for ``front-runner'' learns that acceptable entities are
politicians, even though there is no exact encoding of ``front-runner'' in
Freebase.

The rest of this paper is organized as follows.  In \secref{background}, we
describe the relevant background necessary to understand the models we propose,
and we situate our model in the context of related work.  In \secref{method},
we give a formal description of our models of predicate semantics.  In
\secref{evaluation}, we present experimental results on fill-in-the-blank
natural language query task.  Using web text linked to Freebase as our training
and test data, we show a 63\% relative improvement over prior work, giving a
new state-of-the-art result on this dataset.  In \secref{discussion}, we give a
detailed discussion of the benefits and drawbacks of the models we have
proposed.  We then conclude in \secref{conclusion}.

\section{Background}
\label{sec:background}

Our proposed method is best viewed as the addition of SFE
features~\cite{gardner-2015-sfe} to Krishnamurthy and
Mitchell's~\shortcite{krishnamurthy-2015-semparse-open-vocabulary}
open-vocabulary semantic parser.  Accordingly, in this section we briefly
describe both of these techniques, as well as give a short overview of other
closely related work.

\subsection{Open-vocabulary semantic parsing}
\label{sec:jayant-semparse}

\mattnote{Add a figure here to demonstrate the logical forms.}

This system predicts a denotation $\gamma$, i.e., a set of Freebase
entities, for a natural language text $s$, using the following
probabilistic model:

\begin{align*}
P(\gamma | s) & = \sum_w \sum_\ell P(\gamma | \ell, w) P(w ; \theta, \phi) P(\ell | s)
\end{align*}

In this equation, $P(\ell | s)$ represents a semantic parser that
produces a logical form $\ell$ given the text $s$. This logical form
contains open-vocabulary predicates, as in the ``architect'' example
above. This factor is deterministic, meaning that it assigns
probability one to a single logical form. The second factor $P(w)$
represents a distribution over possible worlds $w$, where a world is
an assignment of truth values to all possible predicate
instances. This distribution can be efficiently represented using a
probabilistic database because each predicate instance is treated as
an independent random variable. The truth probabilities of these
variables are predicted using a matrix factorization approach similiar
to that of Riedel et al., \shortcite{riedel-2013-mf-universal-schema};
$\theta$ and $\phi$ represent the parameters of the matrix
factorization. Finally, $P(\gamma | \ell, w)$ represents the
deterministic evaluation of the logical form $\ell$ on the world $w$
to produce a denotation $\gamma$. This factor is similar to other
semantic parsing work, where it represents the deterministic
evaluation of a query against a given database. Algorithms from the
probabilistic databases community enable us to efficiently compute the
marginal probability that an entity $e$ is in the denotation of a
sentence, $\sum_\gamma P(e \in \gamma | s)$, for a large number of
queries; we refer the reader to (TODO).

The training problem in this model is to learn the parameters $\theta$
and $\phi$ of the probabilistic database. The probability of a
category instance $c(e)$ and relation instance $r(e_1, e_2)$ in the
database is given by:

\begin{align*}
  P(c(e) & = \true{}) & = & \sigma ( \theta_c^T \phi_e ) \\
  P(r(e_1, e_2) & = \true{}) & = & \sigma ( \theta_r^T \phi_{(e_1, e_2)} )
\end{align*}

The parameters $\theta$ represent $k$-dimensional embeddings of each
category and relation predicate, and the parameters $\phi$ represent
$k$-dimensional embeddings of each Freebase entity and entity
pair. The probability of a predicate instance is the inner product of
the corresponding predicate and entity embeddings. Thus, predicates
with nearby embeddings will have similar distributions over the
entities in their denotation.

The parameters $\theta$ and $\phi$ are learned from predicate/entity
co-occurrence counts collected from a large corpus. The training data
consists of query/entity pairs, $\{(\ell^i, e^i)\}$, where the entity
$e^i$ is observed to be an element of $\ell^i$'s denotation. These
pairs are collected by semantically parsing entity-linked sentences
and simplifying the resulting logical forms to identify conjunctions
of predicates where all arguments are bound to specific
entities. Extracting an entity from this conjunction results in a
query/entity pair. The model is trained using a ranking objective that
learns to rank the observed entities for each query above unobserved
entities.

\subsection{Subgraph feature extraction}

Subgraph feature extraction (SFE) is a technique introduced by Gardner and
Mitchell~\shortcite{gardner-2015-sfe} for generating feature matrices over node
pairs in a graph with labeled edges.  Given a pair of nodes in a graph, SFE
performs a search to characterize a subgraph around those two nodes, then
extracts various kinds of features from that subgraph, such as the set of edge
sequences that connect the two nodes.  \mattnote{Give an example graph with
features.}  Gardner and Mitchell used these features to perform link
prediction in the graph, using characteristics of the subgraph around two nodes
to determine if an edge of a particular type should exist between those nodes
(when the graph corresponds to a knowledge base like Freebase, this task is
also known as knowledge base completion). Gardner and Mitchell only used these
features to model formal KB relations, however.

We differ from the work done by Gardner and Mitchell in two ways.  First, we
apply these features to models of natural language, instead of formal KB
relations.  There are some non-trivial problems that need to be solved in order
to make this work, which will be discussed in \secref{method}.  Second, Gardner
and Mitchell only considered generating feature matrices over node \emph{pairs}
in a graph, with which one can model binary relations.  The natural language
queries we wish to answer also involve unary predicates, or categories.  We
thus extend SFE to also generate feature matrices over \emph{nodes} in a graph,
an extension that is straightforward and is also discussed in more detail in
\secref{method}.

\subsection{Other related work}

Some related work on including structured information in distributional representations: Manaal's
retrofitting semantic lexicons, Sebastian Riedel's ``Injecting Logical Background Knowledge into
Embeddings for Relation Extraction'', work on PRA and KBC more generally\ldots Also, the whole line
of work on knowledge base embeddings (RESCAL, TransE, etc.) is at least slightly relevant here, as
is PRA.  And all of the semantic parsing approaches that go from text to a pseudo-logical form,
then to an actual Freebase query (Kwiatkowski, Reddy's TACL paper, Liang?) \mattnote{Actually write
this section}.

\section{Method}
\label{sec:method}

In this section we present our models of predicate semantics.  In
\secref{formal-and-distributional} we describe how we modify Krishnamurthy and
Mitchell's (K\&M's) semantic parser to include features from a formal knowledge base.
In \secref{computing-pmi} we describe how we select those features for each
predicate.  The remaining two sections then describe other improvements we made
to K\&M's semantic parsing model: lexicalizing preposition and noun-compound
predicates, and using the Freebase graph to select candidate entities.

\subsection{Adding SFE features to Jayant's models}
\label{sec:formal-and-distributional}

The modification we make to K\&M's semantic parser is simple.  For each entity
and entity pair in the data we compute an SFE feature vector $\psi(e)$ and
$\psi(e_1, e_2)$.  For each category and relation, we select a subset of the
features to have associated weights $\omega_c$ and $\omega_r$.  We then add the
weights $\omega_c$ and $\omega_r$ as parameters to be learned for each
predicate, and we add the feature vectors $\psi(e)$ and $\psi(e_1, e_2)$ as
observed features for each entity and entity pair.  The augmented category and
relation probabilities are as follows:

\begin{align*}
  P(c(e) & = \true{}) & = & \sigma ( \theta_c^T \phi_e + \omega_c^T \psi_c(e)) \\
  P(r(e_1, e_2) & = \true{}) & = & \sigma ( \theta_r^T \phi_{(e_1, e_2)} + \omega_r^T \psi_r(e_1, e_2) )
\end{align*}
where we have added subscripts on the $\psi$ functions to indicate that each
predicate selects its own set of features from the SFE feature vector $\psi$.

Note that now there are three sets of parameters to be learned by the semantic
parser: (1) $\theta$, low-dimensional distributional vectors trained for each
predicate; (2) $\phi$, low-dimensional distributional vectors trained for each
entity and entity pair; and (3) $\omega$, weights associated with the selected
formal SFE features for each predicate.  All of these parameters are optimized
jointly, using the same ranking objective used by K\&M.

\subsection{Finding relevant features for each predicate}
\label{sec:computing-pmi}

The feature vectors returned by SFE have dimensionality in the tens of
millions.  We do not have near enough data to train that many parameters for
each of our tens of thousands of predicates, nor do we have enough RAM in our
machines to keep that many parameters in memory.  It is thus necessary to
select some small number of informative features for each predicate.  We do
this using pointwise mutual information (PMI).

PMI measures the strength of association between two variables $x$ and $y$; it
is computed as $\log\frac{p(x,y)}{p(x)p(y)}$.  In our case, we want to
associate predicates with SFE features.  However, our SFE features are for each
\emph{entity} and \emph{entity pair}, not each \emph{predicate}.  To associate
SFE features with predicates, we use the training data to find a mapping from
entities and entity pairs to predicates that they have been seen with in the
data.  For example, the phrase ``Italian architect Andrea Palladio'' is
considered a positive training example for the instantiated predicates
$\predicate{architect}(\entity{Andrea Palladio})$ and
$\predicate{architect\_N/N}(\entity{Italy}, \entity{Andrea Palladio})$.  We
then associate the feature vectors for \entity{Andrea Palladio} and
(\entity{Italy}, \entity{Andrea Palladio}) with the predicates
\predicate{architect} and \predicate{architect\_N/N}, respectively.  For each
predicate $\pi$ and feature $f$, we use these counts to calculate the
probabilities $p(\pi, f)$, $p(\pi)$ and $p(f)$.  After removing features with
counts below some threshold, we pick the $k$ features with the highest PMI
values for each predicate to use in our model.

\subsection{Improved logical forms}
\label{sec:better-lfs}

While we largely follow the semantic parsing framework introduced by K\&M, we
made one important change in how logical forms are generated.  For the running
example we have been using this paper, ``Italian architect Andrea Palladio'',
the relation predicate produced by K\&M's system is
$\predicate{N/N}(\entity{Italy}, \entity{Andrea Palladio})$.  \predicate{N/N}
does correctly capture the fact that \entity{Italy} is used syntactically as a
noun modifier of \entity{Andrea Palladio}, but it leaves out the additional
information contained in the syntax that ``architect'' was mediating that
noun-noun dependency.  This means that ``U.S. president Barack Obama'' and
``Italian architect Andrea Palladio'' are both modeled using the same relation
predicate, which leads to a very generic and largely useless model for the
predicate \predicate{N/N}.  By splitting \predicate{N/N} into separate
predicates (\predicate{architect\_N/N} and \predicate{president\_N/N}), we give
the model the opportunity to learn more fine-grained relation semantics, and we
allow our feature selection pipeline to find much more useful features for
relation predicates.  While it would be ideal to use actual dependency
relations to determine which words to add to the predicate in complex noun
phrases, most current parsers treat these kinds of noun phrases as flat, with
all noun modifiers depending directly on the head of the noun phrase.  We
approximate this ideal by simply taking all words in between the two entities
in a noun phrase as the predicate (e.g., ``Illinois attorney general Lisa
Madigan'' would produce the predicate \predicate{attorney\_general\_N/N}).

A similar issue arises with prepositions and possessives.  For a phrase such as
``Barack Obama, president of the U.S.'', K\&M's system would produce the
following relation predicate: $\predicate{of}(\entity{Barack Obama},
\entity{U.S.})$.  Because this same predicate (\predicate{of}) is used for all
instances of this preposition, the predicate is forced to model too much and
becomes overly generic.  When there is a common noun mediating a prepositional
dependency between two entities, as in the phrase above, we add that noun to
the predicate, giving the instance $\predicate{president\_of}(\entity{Barack
Obama}, \entity{U.S.})$.  We do the same for possessives in phrases such as
``Rome, Italy's capital'', producing the predicate \predicate{'s\_capital}.

\subsection{Candidate entity generation}
\label{sec:better-candidates}

One final way in which we improved K\&M's semantic parser is in how candidate
entities are generated during inference.  Because the distributional-only
predicate models used by K\&M's semantic parser need to learn a vector of
parameters for each entity pair, they cannot give a non-zero score to any
entity pair not seen during training time.  When given a query such as
$\lambda(\entity{x}) \predicate{architect\_N/N}(\entity{x}, \entity{Andrea
Palladio})$, K\&M's system only needs to consider the set of entities
$\entity{x}$ that have been seen at training time with \entity{Andrea
Palladio}; all other entities will get a score of 0.

Our predicate models do not have this limitation.  The distributional component
will still give an unseen entity pair a score of 0, as it is identical to K\&M's
model, but the formal component of our simples simply computes an observed
feature vector from Freebase for the entity pair.  As long as both entities
appear in Freebase, our model can compute a feature vector and give a non-zero
score.

While this improvement allows us to handle queries over rare entities much
better than a distributional-only model, it also complicates inference, as we
cannot just score entity pairs seen at training time.  Optimizing every query's
score over all of the millions of entities in Freebase is not computationally
feasible.  To make inference tractable, for a given query we restrict
candidates to all entities seen with the query entities during training (as
done by K\&M), as well as all entities directly connected to the query entities
in Freebase, or connected by a mediator node\footnote{Mediators in Freebase are
used to capture relations with more than two arguments, such as employment
tenure, which has an employer, and employee, a start date, and an end date.}.

For some entities, such as the United States, the set of connected entities can
still be intractably large, so we limit this expansion to only those entities
with fewer than 100 directly connected entities.  Our motivation for this is
that this candidate entity generation is most useful for rarely seen entities,
for which we have few or no related entities seen during training.  These
entities also tend to have relatively few connected entities in Freebase.

\section{Evaluation}
\label{sec:evaluation}

We evaluate our models of predicate semantics on a fill-in-the-blank natural
language query task.  Each test example is a natural language phrase containing
at least two Freebase entities, one of which is held out.  The system must
propose Freebase entities to fill in the blank left by the held out entity, and
the predicted entities are then judged manually for correctness.  We compare
our proposed models, which combine distributional and formal elements, with
distributional-only and formal-only baselines.

\subsection{Data}

We use the dataset introduced by Krishnamurthy and
Mitchell~\shortcite{krishnamurthy-2015-semparse-open-vocabulary}, which
consists of the ClueWeb09 web
corpus\footnote{http://www.lemuproject.org/clueweb09.php} along with Google's
FACC entity linking of that corpus to
Freebase~\cite{gabrilovich-2013-clueweb-entity-linking}.  For training data, 3
million webpages from this corpus were processed with a CCG parser to produce
logical forms, as described in \secref{jayant-semparse}.  This produced 2.1m
predicate instances involving 142k entity pairs and 184k entities.  After
removing infrequently-seen predicates (seen fewer than 6 times), there were 25k
categories and 4.2k relations.\footnote{The differences in numbers reported
here versus those reported by Krishnamurthy and
Mitchell~\shortcite{krishnamurthy-2015-semparse-open-vocabulary} are due to the
improved logical form generation, discussed in \secref{better-lfs}.}

We also used the test set created by Krishnamurthy and Mitchell, which contains
220 queries generated in the same fashion as the training data from a separate
section of ClueWeb.  However, we used this set for intermediate evaluations
while developing our models, and so we also generated another, similar test set
from a different held out section of ClueWeb.  This final test set contains 307
queries.  We report results on both of these test sets below.  All of the data
used in these experiments is available at [url withheld for review].

\subsection{Experiments}

We compare three models in our experiments: a distributional-only model, a
formal-only model, and our proposed model, which is both distributional and
formal.  The distributional-only model is the model proposed by Krishnamurthy
and Mitchell~\shortcite{krishnamurthy-2015-semparse-open-vocabulary} and
described in \secref{jayant-semparse}.  The combined model is the model
described in \secref{sec:formal-and-distributional}, while the formal-only
model removes the distributional component from the combined model (i.e., all
parameters $\theta$ and $\phi$ from the distributional portion of the model are
fixed at zero during training and test).  Note that this formal-only baseline
is also new to this work---we are not aware of any other work that models
language in terms of features derived from a formal knowledge base.

In addition to running experiments comparing these three models on the two test
sets described above, we also evaluated the improvement gained by using our
modified logical forms (\secref{better-lfs}), and our entity proposal mechanism
(\secref{better-candidates}).

\subsection{Methodology}

Given a fill-in-the-blank query such as ``Italian architect \blank{}'', each
system produces a ranked list of 100 candidate entities.  To compare the output
of the systems, we follow a pooled evaluation protocol commonly used in
relation extraction and information
retrieval~\cite{west-2014-kbc-via-qa,riedel-2013-mf-universal-schema}.  We take
the top 30 predictions from each system and manually annotate whether they are
correct, and use those annotations to compute the average precision (AP) and
reciprocal rank (RR) of each system on the query.  Average precision is defined
as $\frac{1}{m}\sum^m_{k=1} \mathrm{Prec}(k) \times \mathrm{Correct}(k)$, where
$\mathrm{Prec}(k)$ is the recision at rank $k$, $\mathrm{Correct}(k)$ is an
indicator function for whether the $k$th answer is correct, and $m$ is number
of returned answers (up to 100, in this evaluation).  Reciprocal rank is
computed by first finding the rank $r$ of the first correct prediction made by a
system.  Reciprocal rank is then $\frac{1}{r}$, ranging from 1 (if the first
prediction is correct) to 0 (if there is no correct answer returned).  In the
tables below we report \emph{mean} average precision (MAP) and reciprocal rank
(MRR), averaged over all of the queries in the test set.  We also report a
weighted version of MAP, where the AP of each query is scaled by the number of
annotated correct answers to the query.

\subsection{Results}

\begin{table}
  \centering
  \begin{tabular}{|l|r|r|}
    \hline
    Method & MAP & MRR \\
    \hline
    Distributional-only & .239 & ?? \\
    Distributional-only (old lfs) & ?? & ?? \\
    \hline
    Formal-only & ?? & ?? \\
    Formal-only (old lfs) & ?? & ?? \\
    \hline
    Combined model & \textbf{.340} & ?? \\
    Combined model (old lfs) & ?? & ?? \\
    \hline
  \end{tabular}
  \caption{Results on the development set for our fill-in-the-blank task.  The
  combined model improves MAP by 42\% compared to the distributional-only
  baseline.  Additionally, the improved logical forms presented in
  \secref{better-lfs} significantly improve all models.}
  \label{tab:dev-results}
\end{table}

\tabref{dev-results} shows the results of our experiments on the development
set (the same as the test set used by Krishnamurthy and
Mitchell~\shortcite{krishnamurthy-2015-semparse-open-vocabulary}).  As can be
seen, the combined model significantly improves performance, giving a 42\%
relative increase in MAP over the distributional-only baseline.  This table
also shows the benefit of using our modified logical forms (see
\secref{better-lfs}); by being able to better model the relations between
entities, MAP of all of the models is significantly improved.\footnote{Because
of the large amount of annotation effort involved, we did not include the
unique results from this run in the evaluation pool.  It is possible that this
had the effect of lowering the MAP score for these methods.  However, the large
majority of the predictions made by these methods were already annotated, and
spot-checking the unannotated ones showed that practically all of the unique
predictions made in this scenario were incorrect, and thus annotating them
would not change the score.}

\begin{table}
  \centering
  \begin{tabular}{|l|r|r|}
    \hline
    Method & MAP & MRR \\
    \hline
    Distributional-only & .231 & ?? \\
    \ \ \ -related entities & .170 & ?? \\
    \hline
    Formal-only & ?? & ?? \\
    \hline
    Combined model & \textbf{.376} & ?? \\
    \hline
  \end{tabular}
  \caption{Results on the final test set for our fill-in-the-blank task.  The
  combined model improves MAP by 63\% relative to the distributional-only
  baseline.  Additionally, allowing the distributional-only model to use
  Freebase to propose candidates improves MAP by 6 points absolute due to
  improved recall.}
  \label{tab:final-results}
\end{table}

\tabref{final-results} shows that these improvements are consistent on the
final test set, as well.  The performance improvement seen by the combined
model is actually larger on this set, going from a MAP of .231 to .376, a 63\%
relative improvement.  In this table, we also show the improvement gained by
allowing the distributional-only model to propose candidate entities that are
related in Freebase (see \secref{better-candidates}).  Even though these
candidates are always ranked at the bottom of the list of predictions (with a
score of 0), the additional recall provided by being able to propose more
entities raises MAP of the distributional-only model from .170 to .231.

\section{Discussion}
\label{sec:discussion}

\mattnote{I will add more detail to some of these examples; I ran out of
time...}

Our model does very well on most predicates that have clear Freebase
correlates: prime ministers, presidents, newspapers, companies, and so on.  The
query ``French newspaper \blank{}'' gets almost perfect average precision.

It does less well on predicates that do not match as closely with Freebase,
such as ``Microsoft head honcho \blank{}'', ``Patriots linebacker \blank{}'',
etc.  The combined model handily beats the distributional-only model in most of
these cases, but it does not have the nearly-perfect AP seen in the more crisp
queries above.  The ``head honcho'' example is particularly informative---this
is a phrase that essentially means ``leader'', and the features learned by the
formal model get some essential hints from Freebase, even though there is no
direct correlate.  The features capture the fact that entities that are
``honchos'' are people, and that they tend to be CEOs and film producers.

For ``Patriots linebacker \blank{}'', the issue is that the features for
\predicate{linebacker\_N/N} are able to capture which athletes play for which
teams, but they cannot distinguish well between what positions those athletes
play, so Patriots quarterbacks, wide receivers, and running backs are scored
highly for this query.  This could be fixed by using more complex features for
relation predicates, instead of just edge sequences, but that would explode the
feature space and make the feature selection process more difficult.  It could
also be fixed by better negative example selection during training.

There are also cases where a predicate has so many possible entities types in
Freebase that the it cannot learn models that are useful for all queries.  A
good example of this is ``\blank{} is a NASA mission''.

The SFE features we used cannot capture time very well.  ``17th century French
mathematician \blank{}''...

Because of the simple logical forms produced by the semantic parser we used, we
cannot model some complex phenomena, such as counting and other things.  This
is an orthogonal issue to the predicate semantics we're concerned with in this
paper, however.

Vice president Al Gore is modeled as $\predicate{vice}(\entity{Al Gore}) \land
\predicate{president}(\entity{Al Gore})$.  Clearly there is room for better
processing of multi-word expressions.

\predicate{head\_honcho\_N/N} was never seen in the training data, so the
phrase ``Microsoft head honcho \blank{}'' has to rely on knowing whether an
entity is a ``honcho'' and what entities are related to Microsoft by an unknown
predicate.  \predicate{honcho\_N/N} \emph{is} seen in the training data,
however; some kind of backoff to simpler phrases would be useful here.

Using connected entities in Freebase as additional candidates during inference
is helpful, but it is not very sophisticated.  It over-generates for a lot of
entities and predicates, and under-generates for others.  A mechanism for
tailoring a search over Freebase to a particular query would be useful, though
that would move the model more into the realm of reinforcement learning.

\section{Conclusion}
\label{sec:conclusion}

\mattnote{TODO}

%\section*{Acknowledgments}

\bibliography{bib}
\bibliographystyle{acl2016}

\end{document}
